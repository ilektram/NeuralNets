{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "from pprint import pformat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1, l2, activity_l2\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(levelname)-8s %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-banner\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"58b521b3-cf89-4679-8f89-843de0c6d141\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.11.1.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#58b521b3-cf89-4679-8f89-843de0c6d141\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "def load_data(input_data):\n",
    "    \"\"\"\n",
    "    Loads data as pandas dataframe\n",
    "    input_data: a single csv file with an ID column\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(input_data, index_col=\"ID\")\n",
    "\n",
    "    # Fill categorical categories with NA value and convert them to the right\n",
    "    # type\n",
    "    for col in df_train.select_dtypes(include=['object']).columns:\n",
    "        df_train[col] = df_train[col].fillna(value='NA', axis=0)\n",
    "        df_train[col] = df_train[col].astype('category')\n",
    "\n",
    "    # Fill the other columns with 0 as the fill value\n",
    "    for col in df_train.select_dtypes(exclude=['category']).columns:\n",
    "        df_train[col] = df_train[col].fillna(value=-1, axis=0)\n",
    "\n",
    "    old_length = df_train.shape[0]\n",
    "    df_train = df_train.dropna(axis=0, how='any')\n",
    "    row_diff = old_length - df_train.shape[0]\n",
    "    logging.debug(\n",
    "        \"Dropped {} rows with NAs {:.1%}\".format(\n",
    "            row_diff,\n",
    "            float(row_diff)/old_length\n",
    "        )\n",
    "    )\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def categorical_to_front(input_df):\n",
    "    cat_columns = list(input_df.select_dtypes(include=['category']).columns)\n",
    "\n",
    "    logging.debug(\"Number of categorical columns: {}\".format(len(cat_columns)))\n",
    "\n",
    "    other_columns = list(input_df.select_dtypes(exclude=['category']).columns)\n",
    "\n",
    "    new_column_order = cat_columns + other_columns\n",
    "    train_df = input_df[new_column_order]\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def categorical_analysis(input_data):\n",
    "    categories = []\n",
    "    for col in input_data.columns:\n",
    "        if str(input_data[col].dtype) == \"category\":\n",
    "            cat = {\n",
    "                \"col_lbl\": col,\n",
    "                \"cat_count\": input_data[col].cat.categories.shape[0]\n",
    "            }\n",
    "            categories.append(cat)\n",
    "    return categories\n",
    "\n",
    "\n",
    "def convert_category_to_columns(input_data, column_name):\n",
    "    if not isinstance(input_data, pd.DataFrame):\n",
    "        raise TypeError(\"Input data must be a Pandas DataFrame\")\n",
    "    if str(input_data[column_name].dtype) != \"category\":\n",
    "        raise RuntimeError(\"Can only run this on categorical columns\")\n",
    "    categories = input_data[column_name].cat.categories\n",
    "\n",
    "    for cat in categories:\n",
    "        new_col_name = \"{col}_{cat}\".format(col=column_name, cat=cat)\n",
    "        input_data[new_col_name] = np.where(\n",
    "            input_data[column_name] == cat,\n",
    "            1,\n",
    "            0\n",
    "        )\n",
    "\n",
    "\n",
    "def convert_categories_to_columns(input_data, cat_thres=130):\n",
    "    cols_to_remove = []\n",
    "    for col in input_data.columns:\n",
    "        if str(input_data[col].dtype) == 'category':\n",
    "            if input_data[col].cat.categories.shape[0] < cat_thres:\n",
    "                convert_category_to_columns(input_data, col)\n",
    "                cols_to_remove.append(col)\n",
    "    input_data.drop(cols_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_categorical(input_df):\n",
    "    \"\"\"\n",
    "    Removes categorical and only leaves numerical variables in the dataframe\n",
    "    Returns a pandas dataframe\n",
    "    input: pandas dataframe with categorical & numerical data\n",
    "    \"\"\"\n",
    "    cat_columns = list(input_df.select_dtypes(include=['category']).columns)\n",
    "\n",
    "    logging.debug(\"Number of categorical columns: {}\".format(len(cat_columns)))\n",
    "\n",
    "    other_columns = list(input_df.select_dtypes(exclude=['category']).columns)\n",
    "\n",
    "    new_column_order = other_columns\n",
    "    train_df = input_df[new_column_order]\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(input_data):\n",
    "    \"\"\"\n",
    "    Splits data to training & testing sets\n",
    "    Splits columns to input & output for training & testing set respectively\n",
    "    Returns ndarrays\n",
    "    input: a pandas dataframe with a \"target\" column\n",
    "    \"\"\"\n",
    "    # Reorder the columns, categorical go first\n",
    "    train_df = categorical_to_front(input_data)\n",
    "    logging.debug(\n",
    "        \"Categories and label counts: {}\".format(\n",
    "            pformat(categorical_analysis(train_df))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    convert_categories_to_columns(train_df)\n",
    "\n",
    "    logging.debug(train_df.get_dtype_counts())\n",
    "\n",
    "    # Temporary\n",
    "    for col in train_df.select_dtypes(include=['category']).columns:\n",
    "        train_df[col] = train_df[col].astype('category').cat.codes\n",
    "\n",
    "    train_inp = train_df.drop('target', axis=1).as_matrix()\n",
    "    train_out = train_df['target'].as_matrix()\n",
    "\n",
    "    logging.debug(\n",
    "        \"Train 0s/1s: {:.2%} / {:.2%}\".format(\n",
    "            1.0 - np.average(train_out),\n",
    "            np.average(train_out)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_inp,\n",
    "                                                        train_out,\n",
    "                                                        test_size=0.33,\n",
    "                                                        random_state=42)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(x_train, y_train, epochs, batch):\n",
    "    \n",
    "    # learning_rate = .1\n",
    "    # sgd = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    error_fun = Adagrad()\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, input_shape=(x_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(output_dim=2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(class_mode='binary', loss='binary_crossentropy', optimizer=error_fun)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode='auto'\n",
    "    )    \n",
    "    \n",
    "    hist = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        nb_epoch=epochs,\n",
    "        batch_size=batch,\n",
    "        validation_split=0.1,\n",
    "        show_accuracy=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model2(x_train, y_train, epochs, batch):\n",
    "    \n",
    "    # learning_rate = .1\n",
    "    # sgd = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    error_fun = Adagrad()\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512, input_shape=(x_train.shape[1],)))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(output_dim=2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(class_mode='binary', loss='binary_crossentropy', optimizer=error_fun)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode='auto'\n",
    "    )    \n",
    "    \n",
    "    hist = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        nb_epoch=epochs,\n",
    "        batch_size=batch,\n",
    "        validation_split=0.1,\n",
    "        show_accuracy=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epochs_perf_plot(hist):\n",
    "    \"\"\"\n",
    "    Create plot of model performance by epoch\n",
    "    input: nn history object, # epochs\n",
    "    returns bokeh line plot\n",
    "    \"\"\"\n",
    "    epochs = len(hist.history['acc'])\n",
    "    p = figure(title=\"Model Performance (Training Set)\", plot_width=600, plot_height=600)\n",
    "\n",
    "    p.line(x=range(0, epochs), y=hist.history['loss'],\n",
    "           color=\"firebrick\", line_width=4, legend=\"Loss\")\n",
    "    p.line(x=range(0, epochs), y=hist.history['acc'],\n",
    "           color=\"navy\", line_width=4, legend=\"Accuracy\")\n",
    "    \n",
    "    p.legend.orientation = \"bottom_left\"\n",
    "    p.xaxis.axis_label = \"Epoch\"\n",
    "    \n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def undersample(X, class_column):\n",
    "    \"\"\"\n",
    "    Undersamples a dataset to obtain equal number of classes from imbalanced data\n",
    "    input: initial dataset as pandas dataframe, column with class labels as string\n",
    "    \"\"\"\n",
    "    counts = X[class_column].value_counts(ascending=True)\n",
    "    print(\"The frequency of each class: {}.\".format(X[class_column].value_counts(normalize=True)))\n",
    "    classes = pd.unique(X[class_column].ravel())\n",
    "    l = []\n",
    "    for value in classes:\n",
    "        class_indices = X[X[class_column] == value].index\n",
    "        random_index = random.sample(class_indices, counts[0])\n",
    "        l.extend(random_index)\n",
    "    return X.ix[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data & split into testing & training set\n",
    "train_df = load_data(\"train.csv\")\n",
    "x_train, x_test, y_train, y_test = train_test(train_df)\n",
    "\n",
    "logging.debug(\"SHAPES: IN Train [{}], Test [{}]\".format(x_train.shape, x_test.shape))\n",
    "logging.debug(\"SHAPES: OUT Train [{}], Test [{}]\".format(y_train.shape, y_test.shape))\n",
    "\n",
    "# Create NN for 2-layer unidimensional regression\n",
    "batch = 1024\n",
    "epochs = 100\n",
    "\n",
    "model, hist = nn_model(x_train, y_train, epochs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot model training performance\n",
    "epochs_perf_plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test set predictions\n",
    "predicted = model.predict(x_test)\n",
    "logging.info(\"Predicted 0s/1s: {:.2%} {:.2%}\".format(np.average(predicted[:, 0]), np.average(predicted[:, 1])))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, show_accuracy=True, batch_size=batch)\n",
    "\n",
    "print('Test score (log loss): {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Precision & Recall metrics on test set\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test[:,1], predicted[:,1])\n",
    "# Plot PRC\n",
    "p = figure(title=\"Model Metrics (PRC)\", plot_width=600, plot_height=600)\n",
    "\n",
    "p.line(x=recall, y=precision, color=\"firebrick\", line_width=4)\n",
    "p.xaxis.axis_label = \"Recall\"\n",
    "p.yaxis.axis_label = \"Precision\"\n",
    "    \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get true positive rate & false positive rate\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test[:,1], predicted[:,1])\n",
    "# Plot ROC\n",
    "p = figure(title=\"Model Metrics (ROC)\", plot_width=600, plot_height=600)\n",
    "\n",
    "p.line(x=fpr, y=tpr, color=\"navy\", line_width=4)\n",
    "p.xaxis.axis_label = \"False Positive Rate\"\n",
    "p.yaxis.axis_label = \"True Positive Rate\"\n",
    "    \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute AUC\n",
    "auc = metrics.roc_auc_score(y_test[:,1], predicted[:,1])\n",
    "print(\"The AUC score is: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NN with undersampled data (# class 1 == # class 0)\n",
    "data_df = load_data(\"train.csv\")\n",
    "data_df = undersample(data_df, \"target\")\n",
    "x_train, x_test, y_train, y_test = train_test(train_df)\n",
    "\n",
    "logging.debug(\"SHAPES: IN Train [{}], Test [{}]\".format(x_train.shape, x_test.shape))\n",
    "logging.debug(\"SHAPES: OUT Train [{}], Test [{}]\".format(y_train.shape, y_test.shape))\n",
    "\n",
    "model, hist = nn_model(x_train, y_train, epochs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot undersampled model training performance\n",
    "epochs_perf_plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Undersampled Test set predictions\n",
    "predicted = model.predict(x_test)\n",
    "logging.info(\"Predicted 0s/1s: {:.2%} {:.2%}\".format(np.average(predicted[:, 0]), np.average(predicted[:, 1])))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, show_accuracy=True, batch_size=batch)\n",
    "\n",
    "print('Test score (log loss): {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model3, hist3 = nn_model2(x_train2, y_train2, epochs, batch)\n",
    "\n",
    "# Plot model 3 training performance\n",
    "epochs_perf_plot(hist3)\n",
    "\n",
    "# Test set 3 predictions\n",
    "predicted3 = model3.predict(x_test2)\n",
    "logging.info(\"Predicted 0s/1s: {:.2%} {:.2%}\".format(np.average(predicted3[:, 0]), np.average(predicted3[:, 1])))\n",
    "\n",
    "score3 = model3.evaluate(x_test2, y_test2, show_accuracy=True, batch_size=batch)\n",
    "\n",
    "print('Test score (log loss): {}'.format(score3[0]))\n",
    "print('Test accuracy: {}'.format(score3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = remove_categorical(load_data(\"train.csv\"))\n",
    "train_df = undersample(train_df, \"target\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test(train_df)\n",
    "\n",
    "logging.debug(\"SHAPES: IN Train [{}], Test [{}]\".format(x_train.shape, x_test.shape))\n",
    "logging.debug(\"SHAPES: OUT Train [{}], Test [{}]\".format(y_train.shape, y_test.shape))\n",
    "\n",
    "model, hist = nn_model2(x_train, y_train, epochs, batch)\n",
    "# Plot model training performance\n",
    "epochs_perf_plot(hist)\n",
    "# Test set predictions\n",
    "predicted = model.predict(x_test)\n",
    "logging.info(\"Predicted 0s/1s: {:.2%} {:.2%}\".format(np.average(predicted[:, 0]), np.average(predicted[:, 1])))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, show_accuracy=True, batch_size=batch)\n",
    "\n",
    "print('Test score (log loss): {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = load_data(\"train.csv\")\n",
    "train_df = undersample(train_df, \"target\")\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "X = train_df.drop(\"target\", axis=1).drop(\"v22\", axis=1)\n",
    "dv = DictVectorizer()\n",
    "\n",
    "X = dv.fit_transform(X.T.to_dict().values())\n",
    "y = train_df[\"target\"].as_matrix()\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=5)\n",
    "\n",
    "X = lda.fit_transform(X.toarray(), y)\n",
    "\n",
    "train_df = pd.DataFrame(X)\n",
    "train_df[\"target\"] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test(train_df)\n",
    "\n",
    "logging.debug(\"SHAPES: IN Train [{}], Test [{}]\".format(x_train.shape, x_test.shape))\n",
    "logging.debug(\"SHAPES: OUT Train [{}], Test [{}]\".format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run model on LDA output\n",
    "model, hist = nn_model(x_train, y_train, epochs, batch)\n",
    "# Plot model training performance\n",
    "epochs_perf_plot(hist)\n",
    "# Test set predictions\n",
    "predicted = model.predict(x_test)\n",
    "logging.info(\"Predicted 0s/1s: {:.2%} {:.2%}\".format(np.average(predicted[:, 0]), np.average(predicted[:, 1])))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, show_accuracy=True, batch_size=batch)\n",
    "\n",
    "print('Test score (log loss): {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Accuracy training SVC(linear): 0.761368235784 (+/- 2.81154472936e-05)\n",
      "Accuracy test set SVC(linear): 0.760854583046\n",
      "Logloss test set SVC(linear): 8.25998130209\n",
      "Accuracy training SVC(rbf): 0.772596112158 (+/- 0.0014309906858)\n",
      "Accuracy test set SVC(rbf): 0.773074272385\n",
      "Logloss test set SVC(rbf): 7.83790277176\n",
      "Accuracy training SVC(poly): 0.766538289118 (+/- 0.000574905518871)\n",
      "Accuracy test set SVC(poly): 0.768223506335\n",
      "Logloss test set SVC(poly): 8.00545855216\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create SVM\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#train_df = load_data(\"train.csv\")\n",
    "#x_train, x_test, y_train, y_test = train_test(train_df)\n",
    "\n",
    "train_df = load_data(\"train.csv\")\n",
    "\n",
    "X = train_df.drop(\"target\", axis=1).drop(\"v22\", axis=1)\n",
    "\n",
    "X = pd.get_dummies(X).as_matrix()\n",
    "\n",
    "y = train_df[\"target\"].as_matrix()\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=10)\n",
    "\n",
    "X = lda.fit_transform(X, y)\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(X)\n",
    "train_df[\"target\"] = y\n",
    "\n",
    "x_train, x_test, y_train_d, y_test_d = train_test(train_df)\n",
    "\n",
    "y_train = y_train_d[:, 1]\n",
    "y_test = y_test_d[:, 1]\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C, cache_size=1024)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C, cache_size=1024)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C, cache_size=1024)\n",
    "print(\"ok\")\n",
    "\n",
    "svc_score = cross_val_score(svc, x_train, y_train, cv=3)\n",
    "svc_pred = svc.fit(x_train, y_train)\n",
    "svc_loss = metrics.log_loss(y_test, svc_pred.predict(x_test))\n",
    "print(\"Accuracy training SVC(linear): {} (+/- {})\".format(svc_score.mean(), svc_score.std() * 2))\n",
    "print(\"Accuracy test set SVC(linear): {}\".format(svc_pred.score(x_test, y_test)))\n",
    "print(\"Logloss test set SVC(linear): {}\".format(svc_loss))\n",
    "\n",
    "rbf_svc_score = cross_val_score(rbf_svc, x_train, y_train, cv=3)\n",
    "rbf_svc_pred = rbf_svc.fit(x_train, y_train)\n",
    "rbf_svc_loss = metrics.log_loss(y_test, rbf_svc_pred.predict(x_test))\n",
    "print(\"Accuracy training SVC(rbf): {} (+/- {})\".format(rbf_svc_score.mean(), rbf_svc_score.std() * 2))\n",
    "print(\"Accuracy test set SVC(rbf): {}\".format(rbf_svc.score(x_test, y_test)))\n",
    "print(\"Logloss test set SVC(rbf): {}\".format(rbf_svc_loss))\n",
    "      \n",
    "poly_svc_score = cross_val_score(poly_svc, x_train, y_train, cv=3)\n",
    "poly_svc_pred = poly_svc.fit(x_train, y_train)\n",
    "poly_svc_loss = metrics.log_loss(y_test, poly_svc_pred.predict(x_test))\n",
    "print(\"Accuracy training SVC(poly): {} (+/- {})\".format(poly_svc_score.mean(), poly_svc_score.std() * 2))\n",
    "print(\"Accuracy test set SVC(poly): {}\".format(poly_svc.score(x_test, y_test)))\n",
    "print(\"Logloss test set SVC(poly): {}\".format(poly_svc_loss))\n",
    "      \n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          target        v1        v2        v4        v5        v6        v7  \\\n",
      "target  1.000000 -0.018843 -0.003288 -0.002915 -0.014713 -0.011823 -0.011511   \n",
      "v1     -0.018843  1.000000  0.696461  0.779529  0.755411  0.818256  0.835146   \n",
      "v2     -0.003288  0.696461  1.000000  0.917658  0.827596  0.856378  0.896183   \n",
      "v4     -0.002915  0.779529  0.917658  1.000000  0.882361  0.946102  0.955000   \n",
      "v5     -0.014713  0.755411  0.827596  0.882361  1.000000  0.891806  0.878692   \n",
      "v6     -0.011823  0.818256  0.856378  0.946102  0.891806  1.000000  0.948777   \n",
      "v7     -0.011511  0.835146  0.896183  0.955000  0.878692  0.948777  1.000000   \n",
      "v8     -0.018114  0.478706  0.282083  0.412737  0.481223  0.471503  0.419458   \n",
      "v9     -0.020321  0.811641  0.846393  0.889921  0.851149  0.923412  0.925890   \n",
      "v10     0.147620  0.016779  0.043768  0.047789  0.039626  0.036445  0.031098   \n",
      "v11    -0.017669  0.836102  0.888219  0.947135  0.902813  0.965504  0.962327   \n",
      "v12     0.049938  0.030596  0.052794  0.057743  0.055315  0.049839  0.044303   \n",
      "v13    -0.023885  0.806561  0.758232  0.836684  0.844294  0.891104  0.859482   \n",
      "v14     0.130305  0.035072  0.064813  0.078271  0.068727  0.061228  0.051806   \n",
      "v15    -0.025617  0.808250  0.778417  0.840019  0.832327  0.889203  0.877603   \n",
      "v16    -0.019758  0.726872  0.806217  0.839345  0.815165  0.864038  0.875042   \n",
      "v17     0.002765  0.701106  0.893304  0.940392  0.840510  0.866025  0.862985   \n",
      "v18    -0.015023  0.780386  0.741124  0.840036  0.833233  0.884409  0.861252   \n",
      "v19    -0.019668  0.828695  0.841211  0.907622  0.877444  0.935045  0.930220   \n",
      "v20    -0.016323  0.836168  0.897426  0.956381  0.909213  0.970482  0.967555   \n",
      "v21     0.049773  0.027377  0.039555  0.051921  0.048543  0.042683  0.033621   \n",
      "v23    -0.029467  0.365615  0.146130  0.169373  0.265181  0.254411  0.265044   \n",
      "v25    -0.016000  0.488453  0.301915  0.439765  0.502401  0.482777  0.430538   \n",
      "v26    -0.011297  0.847760  0.877027  0.955536  0.888588  0.950834  0.972897   \n",
      "v27    -0.010418  0.810907  0.871529  0.942656  0.850114  0.933708  0.976523   \n",
      "v28    -0.020745  0.719442  0.671399  0.757593  0.808336  0.814491  0.778592   \n",
      "v29    -0.014326  0.807325  0.893311  0.946659  0.909552  0.960051  0.943128   \n",
      "v32    -0.025043  0.813323  0.793287  0.852114  0.849539  0.904192  0.887120   \n",
      "v33    -0.031566  0.863325  0.680872  0.762806  0.774232  0.843288  0.841154   \n",
      "v34     0.093937 -0.069284 -0.037162 -0.045326 -0.075657 -0.068079 -0.055925   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "v97    -0.014832  0.793368  0.835912  0.921048  0.883396  0.933993  0.901499   \n",
      "v98    -0.012567  0.729987  0.808409  0.862103  0.861146  0.850268  0.894182   \n",
      "v99    -0.010138  0.788918  0.874665  0.940387  0.898150  0.949260  0.931815   \n",
      "v100    0.001027  0.582544  0.857435  0.822794  0.705293  0.750954  0.822763   \n",
      "v101   -0.000854  0.733755  0.927553  0.963014  0.875886  0.906724  0.911642   \n",
      "v102   -0.009688  0.720753  0.665062  0.791943  0.802433  0.797447  0.756580   \n",
      "v103   -0.021365  0.835180  0.804493  0.877267  0.846538  0.908074  0.931631   \n",
      "v104   -0.023198  0.804975  0.772199  0.845263  0.847018  0.889130  0.869551   \n",
      "v105   -0.013891  0.488031  0.323545  0.460226  0.516946  0.490836  0.440908   \n",
      "v106   -0.003807  0.767176  0.944868  0.983338  0.895309  0.940864  0.945658   \n",
      "v108   -0.008901  0.730633  0.819772  0.887322  0.946489  0.865528  0.852231   \n",
      "v109    0.003538  0.542431  0.629215  0.710509  0.735623  0.658094  0.627779   \n",
      "v111   -0.030534  0.849087  0.662733  0.747242  0.758435  0.831503  0.825570   \n",
      "v114    0.075563 -0.070668 -0.044848 -0.054746 -0.084172 -0.074744 -0.060465   \n",
      "v115   -0.015664  0.810200  0.864610  0.926256  0.876671  0.942824  0.941462   \n",
      "v116   -0.012290  0.832425  0.861912  0.946933  0.869456  0.941527  0.976779   \n",
      "v117   -0.012536  0.693114  0.712324  0.807236  0.905793  0.813068  0.773607   \n",
      "v118   -0.016440  0.836904  0.826501  0.914401  0.882621  0.934402  0.901496   \n",
      "v119   -0.047285  0.553432  0.280384  0.328428  0.474624  0.463026  0.459076   \n",
      "v120   -0.016432  0.773696  0.771561  0.845830  0.771554  0.859374  0.860557   \n",
      "v121   -0.032371  0.833913  0.644662  0.734449  0.738341  0.813991  0.818557   \n",
      "v122   -0.016560  0.779191  0.859893  0.898343  0.853007  0.918677  0.911793   \n",
      "v123   -0.041798  0.736141  0.480459  0.560283  0.650292  0.672758  0.668797   \n",
      "v124   -0.015153  0.483376  0.332176  0.452215  0.507687  0.480960  0.434945   \n",
      "v126   -0.017552  0.820709  0.836000  0.900337  0.860008  0.916835  0.943568   \n",
      "v127   -0.010555  0.731856  0.765842  0.866599  0.746113  0.855492  0.867650   \n",
      "v128   -0.005935  0.701899  0.798587  0.867857  0.923381  0.836852  0.821211   \n",
      "v129    0.142141  0.003205  0.040378  0.040359  0.028103  0.023502  0.021539   \n",
      "v130   -0.034563  0.785847  0.590388  0.668809  0.711708  0.767455  0.726638   \n",
      "v131   -0.004473  0.914366  0.790871  0.854478  0.788293  0.839100  0.862266   \n",
      "\n",
      "              v8        v9       v10    ...         v121      v122      v123  \\\n",
      "target -0.018114 -0.020321  0.147620    ...    -0.032371 -0.016560 -0.041798   \n",
      "v1      0.478706  0.811641  0.016779    ...     0.833913  0.779191  0.736141   \n",
      "v2      0.282083  0.846393  0.043768    ...     0.644662  0.859893  0.480459   \n",
      "v4      0.412737  0.889921  0.047789    ...     0.734449  0.898343  0.560283   \n",
      "v5      0.481223  0.851149  0.039626    ...     0.738341  0.853007  0.650292   \n",
      "v6      0.471503  0.923412  0.036445    ...     0.813991  0.918677  0.672758   \n",
      "v7      0.419458  0.925890  0.031098    ...     0.818557  0.911793  0.668797   \n",
      "v8      1.000000  0.421302  0.011240    ...     0.530507  0.410312  0.529779   \n",
      "v9      0.421302  1.000000  0.022691    ...     0.852117  0.983635  0.717603   \n",
      "v10     0.011240  0.022691  1.000000    ...    -0.001472  0.030687 -0.019432   \n",
      "v11     0.469626  0.953416  0.031240    ...     0.862537  0.940326  0.730555   \n",
      "v12     0.023342  0.038828  0.898680    ...     0.016507  0.045112  0.000229   \n",
      "v13     0.578610  0.894968  0.022580    ...     0.878179  0.878184  0.782403   \n",
      "v14     0.030099  0.034343  0.685335    ...     0.007853  0.045402 -0.011486   \n",
      "v15     0.508352  0.944523  0.016592    ...     0.888581  0.932030  0.781259   \n",
      "v16     0.398014  0.901750  0.019943    ...     0.773828  0.849629  0.683953   \n",
      "v17     0.387112  0.777072  0.058202    ...     0.617921  0.802629  0.468148   \n",
      "v18     0.492105  0.859728  0.025895    ...     0.817541  0.843940  0.683511   \n",
      "v19     0.500143  0.929497  0.025003    ...     0.878322  0.910235  0.760984   \n",
      "v20     0.469996  0.958137  0.034108    ...     0.846542  0.950601  0.708016   \n",
      "v21     0.022183  0.022480  0.265726    ...     0.008002  0.029149 -0.000530   \n",
      "v23     0.298436  0.299512 -0.026709    ...     0.501035  0.243798  0.735889   \n",
      "v25     0.957416  0.419564  0.016254    ...     0.514035  0.414458  0.518900   \n",
      "v26     0.477829  0.905009  0.036273    ...     0.803066  0.900875  0.668025   \n",
      "v27     0.412728  0.909741  0.030388    ...     0.808915  0.894430  0.649223   \n",
      "v28     0.475091  0.791784  0.022209    ...     0.766774  0.775432  0.673510   \n",
      "v29     0.468806  0.945388  0.039230    ...     0.794304  0.951553  0.664081   \n",
      "v32     0.507834  0.956319  0.019047    ...     0.881922  0.936717  0.774922   \n",
      "v33     0.535214  0.885610  0.002093    ...     0.987556  0.821488  0.889415   \n",
      "v34    -0.070859 -0.077598  0.245512    ...    -0.092912 -0.071093 -0.104799   \n",
      "...          ...       ...       ...    ...          ...       ...       ...   \n",
      "v97     0.529876  0.890660  0.038183    ...     0.795440  0.894436  0.681971   \n",
      "v98     0.371022  0.849784  0.024386    ...     0.751046  0.831338  0.613944   \n",
      "v99     0.468176  0.905293  0.042299    ...     0.773358  0.910763  0.628875   \n",
      "v100    0.093977  0.757633  0.039621    ...     0.536918  0.756894  0.343216   \n",
      "v101    0.379746  0.841588  0.052503    ...     0.666956  0.858744  0.512389   \n",
      "v102    0.574001  0.728226  0.043960    ...     0.687279  0.734710  0.623949   \n",
      "v103    0.502914  0.894123  0.017542    ...     0.904388  0.858727  0.797367   \n",
      "v104    0.556776  0.884434  0.021632    ...     0.872068  0.865296  0.776254   \n",
      "v105    0.938682  0.426527  0.018739    ...     0.500725  0.426400  0.492810   \n",
      "v106    0.397568  0.887043  0.051167    ...     0.708410  0.902631  0.543272   \n",
      "v108    0.433122  0.804312  0.046443    ...     0.681346  0.814378  0.596993   \n",
      "v109    0.395442  0.550892  0.051658    ...     0.453869  0.580175  0.409218   \n",
      "v111    0.522920  0.874916  0.001206    ...     0.975044  0.811864  0.875438   \n",
      "v114   -0.074737 -0.080384 -0.103633    ...    -0.089242 -0.076253 -0.096700   \n",
      "v115    0.449951  0.934835  0.031288    ...     0.831515  0.926833  0.699295   \n",
      "v116    0.472718  0.894192  0.030907    ...     0.827434  0.879057  0.690055   \n",
      "v117    0.536735  0.729472  0.044233    ...     0.658963  0.739957  0.609679   \n",
      "v118    0.542894  0.895757  0.035746    ...     0.819516  0.895026  0.710257   \n",
      "v119    0.419031  0.522182 -0.041813    ...     0.708778  0.457567  0.885941   \n",
      "v120    0.469330  0.885872  0.022682    ...     0.832538  0.865627  0.676935   \n",
      "v121    0.530507  0.852117 -0.001472    ...     1.000000  0.789371  0.893065   \n",
      "v122    0.410312  0.983635  0.030687    ...     0.789371  1.000000  0.649891   \n",
      "v123    0.529779  0.717603 -0.019432    ...     0.893065  0.649891  1.000000   \n",
      "v124    0.835329  0.425900  0.015243    ...     0.494246  0.426823  0.487989   \n",
      "v126    0.485996  0.895298  0.023029    ...     0.859973  0.873211  0.740302   \n",
      "v127    0.410544  0.847907  0.024187    ...     0.762278  0.833294  0.592677   \n",
      "v128    0.450891  0.765129  0.047329    ...     0.645966  0.780527  0.564490   \n",
      "v129   -0.003341  0.006765  0.491903    ...    -0.016217  0.013957 -0.033269   \n",
      "v130    0.548897  0.816306 -0.002117    ...     0.946706  0.756183  0.883464   \n",
      "v131    0.394323  0.790096  0.036439    ...     0.679416  0.796899  0.570427   \n",
      "\n",
      "            v124      v126      v127      v128      v129      v130      v131  \n",
      "target -0.015153 -0.017552 -0.010555 -0.005935  0.142141 -0.034563 -0.004473  \n",
      "v1      0.483376  0.820709  0.731856  0.701899  0.003205  0.785847  0.914366  \n",
      "v2      0.332176  0.836000  0.765842  0.798587  0.040378  0.590388  0.790871  \n",
      "v4      0.452215  0.900337  0.866599  0.867857  0.040359  0.668809  0.854478  \n",
      "v5      0.507687  0.860008  0.746113  0.923381  0.028103  0.711708  0.788293  \n",
      "v6      0.480960  0.916835  0.855492  0.836852  0.023502  0.767455  0.839100  \n",
      "v7      0.434945  0.943568  0.867650  0.821211  0.021539  0.726638  0.862266  \n",
      "v8      0.835329  0.485996  0.410544  0.450891 -0.003341  0.548897  0.394323  \n",
      "v9      0.425900  0.895298  0.847907  0.765129  0.006765  0.816306  0.790096  \n",
      "v10     0.015243  0.023029  0.024187  0.047329  0.491903 -0.002117  0.036439  \n",
      "v11     0.481399  0.953736  0.856305  0.837319  0.016633  0.826554  0.841454  \n",
      "v12     0.025428  0.038228  0.036076  0.058699  0.420586  0.016228  0.045451  \n",
      "v13     0.566332  0.930163  0.751423  0.774448  0.006173  0.896397  0.753537  \n",
      "v14     0.044907  0.042507  0.037320  0.089370  0.396813  0.008298  0.062125  \n",
      "v15     0.489861  0.920005  0.804938  0.744512  0.001275  0.877788  0.757374  \n",
      "v16     0.386080  0.885630  0.769389  0.727203  0.011268  0.771862  0.711376  \n",
      "v17     0.484598  0.805835  0.749550  0.889482  0.054094  0.572398  0.809708  \n",
      "v18     0.481804  0.859089  0.814214  0.748135  0.014480  0.769985  0.755515  \n",
      "v19     0.499935  0.964999  0.830110  0.808132  0.010721  0.865807  0.812092  \n",
      "v20     0.484811  0.943693  0.860416  0.847947  0.020376  0.807903  0.850583  \n",
      "v21     0.034430  0.028981  0.020779  0.065508  0.122639  0.011386  0.043558  \n",
      "v23     0.256973  0.335804  0.192216  0.215253 -0.034720  0.519540  0.225228  \n",
      "v25     0.862017  0.482550  0.413831  0.522253 -0.000136  0.528615  0.419631  \n",
      "v26     0.507300  0.919487  0.833967  0.858736  0.025011  0.736472  0.873475  \n",
      "v27     0.420470  0.917722  0.912910  0.789993  0.024150  0.704992  0.839279  \n",
      "v28     0.461940  0.803948  0.706752  0.699241  0.006948  0.752256  0.687592  \n",
      "v29     0.495867  0.920009  0.801969  0.859873  0.023332  0.778509  0.837527  \n",
      "v32     0.495658  0.917198  0.786358  0.764015  0.002977  0.882721  0.762887  \n",
      "v33     0.505689  0.882672  0.751221  0.681896 -0.013815  0.961511  0.719044  \n",
      "v34    -0.064110 -0.073451 -0.043652 -0.058289  0.108691 -0.105562 -0.045633  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "v97     0.561821  0.891334  0.787465  0.860423  0.023611  0.786645  0.809344  \n",
      "v98     0.368109  0.849812  0.874493  0.784359  0.020762  0.657116  0.757276  \n",
      "v99     0.480022  0.910572  0.804816  0.847385  0.031152  0.747802  0.827614  \n",
      "v100    0.127812  0.751009  0.683784  0.640931  0.033338  0.464849  0.691033  \n",
      "v101    0.432475  0.860989  0.759895  0.883353  0.044503  0.628602  0.827765  \n",
      "v102    0.662047  0.749308  0.656476  0.844217  0.024982  0.674249  0.719768  \n",
      "v103    0.488723  0.956775  0.812241  0.772864  0.005353  0.859353  0.796520  \n",
      "v104    0.540278  0.943000  0.755129  0.776318  0.007475  0.881877  0.761331  \n",
      "v105    0.913971  0.484747  0.422833  0.560312  0.003262  0.512235  0.431881  \n",
      "v106    0.450016  0.892097  0.826875  0.873529  0.043319  0.657449  0.852613  \n",
      "v108    0.522683  0.817558  0.713407  0.988531  0.034830  0.654970  0.788555  \n",
      "v109    0.526441  0.590800  0.495349  0.910439  0.046603  0.448538  0.627066  \n",
      "v111    0.493691  0.863863  0.740419  0.665159 -0.014674  0.947127  0.703510  \n",
      "v114   -0.068731 -0.076043 -0.047247 -0.068650 -0.042953 -0.102142 -0.051779  \n",
      "v115    0.461635  0.926625  0.842872  0.813651  0.016845  0.789872  0.823156  \n",
      "v116    0.490456  0.928629  0.867661  0.832686  0.022245  0.739685  0.849198  \n",
      "v117    0.573364  0.764644  0.644152  0.923690  0.033742  0.650691  0.725102  \n",
      "v118    0.571501  0.896454  0.788454  0.855903  0.020770  0.814258  0.834440  \n",
      "v119    0.386524  0.535195  0.377238  0.396439 -0.054705  0.715905  0.384006  \n",
      "v120    0.460518  0.864813  0.847571  0.712901  0.015303  0.789588  0.741065  \n",
      "v121    0.494246  0.859973  0.762278  0.645966 -0.016217  0.946706  0.679416  \n",
      "v122    0.426823  0.873211  0.833294  0.780527  0.013957  0.756183  0.796899  \n",
      "v123    0.487989  0.740302  0.592677  0.564490 -0.033269  0.883464  0.570427  \n",
      "v124    1.000000  0.477847  0.401732  0.558925  0.004211  0.512117  0.428393  \n",
      "v126    0.477847  1.000000  0.820217  0.786370  0.011372  0.815985  0.810091  \n",
      "v127    0.401732  0.820217  1.000000  0.685680  0.018875  0.657286  0.750046  \n",
      "v128    0.558925  0.786370  0.685680  1.000000  0.037473  0.622427  0.767655  \n",
      "v129    0.004211  0.011372  0.018875  0.037473  1.000000 -0.019133  0.025985  \n",
      "v130    0.512117  0.815985  0.657286  0.622427 -0.019133  1.000000  0.631253  \n",
      "v131    0.428393  0.810091  0.750046  0.767655  0.025985  0.631253  1.000000  \n",
      "\n",
      "[113 rows x 113 columns]\n",
      "    Year   Jan   Feb   Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec  \\\n",
      "0   1948   4.0   4.7   4.5  4.0  3.4  3.9  3.9  3.6  3.4  2.9  3.3  3.6   \n",
      "1   1949   5.0   5.8   5.6  5.4  5.7  6.4  7.0  6.3  5.9  6.1  5.7  6.0   \n",
      "2   1950   7.6   7.9   7.1  6.0  5.3  5.6  5.3  4.1  4.0  3.3  3.8  3.9   \n",
      "3   1951   4.4   4.2   3.8  3.2  2.9  3.4  3.3  2.9  3.0  2.8  3.2  2.9   \n",
      "4   1952   3.7   3.8   3.3  3.0  2.9  3.2  3.3  3.1  2.7  2.4  2.5  2.5   \n",
      "5   1953   3.4   3.2   2.9  2.8  2.5  2.7  2.7  2.4  2.6  2.5  3.2  4.2   \n",
      "6   1954   5.7   6.3   6.4  6.1  5.7  5.7  5.7  5.4  5.3  4.6  4.9  4.8   \n",
      "7   1955   5.8   5.7   5.2  4.9  4.2  4.4  4.0  3.8  3.5  3.4  3.8  3.9   \n",
      "8   1956   4.7   4.8   4.7  4.1  4.2  4.7  4.4  3.7  3.4  3.1  3.9  4.0   \n",
      "9   1957   4.9   4.7   4.3  4.0  3.9  4.6  4.1  3.7  3.7  3.6  4.6  5.0   \n",
      "10  1958   6.8   7.7   7.7  7.5  7.1  7.6  7.4  6.7  6.0  5.5  5.6  6.0   \n",
      "11  1959   7.0   7.0   6.4  5.2  4.9  5.4  5.2  4.8  4.7  4.7  5.3  5.1   \n",
      "12  1960   6.1   5.7   6.1  5.2  4.8  5.8  5.5  5.2  4.7  5.0  5.6  6.4   \n",
      "13  1961   7.7   8.1   7.7  7.0  6.6  7.3  6.9  6.2  5.8  5.5  5.6  5.8   \n",
      "14  1962   6.7   6.5   6.2  5.5  5.1  5.9  5.3  5.3  4.9  4.5  5.3  5.3   \n",
      "15  1963   6.6   6.9   6.3  5.6  5.5  6.2  5.6  5.2  4.8  4.7  5.3  5.3   \n",
      "16  1964   6.4   6.2   5.9  5.3  4.8  5.9  4.9  4.8  4.5  4.4  4.5  4.7   \n",
      "17  1965   5.5   5.7   5.1  4.7  4.3  5.3  4.5  4.2  3.8  3.6  3.9  3.7   \n",
      "18  1966   4.4   4.2   4.0  3.6  3.7  4.6  3.9  3.6  3.3  3.2  3.4  3.5   \n",
      "19  1967   4.2   4.2   3.9  3.5  3.2  4.6  4.1  3.7  3.7  3.8  3.7  3.5   \n",
      "20  1968   4.0   4.2   3.8  3.2  2.9  4.5  4.0  3.5  3.3  3.2  3.3  3.1   \n",
      "21  1969   3.7   3.7   3.5  3.2  2.9  4.1  3.8  3.5  3.7  3.5  3.3  3.2   \n",
      "22  1970   4.2   4.7   4.6  4.3  4.1  5.6  5.3  5.0  5.2  5.1  5.5  5.6   \n",
      "23  1971   6.6   6.6   6.3  5.7  5.3  6.5  6.2  5.9  5.8  5.4  5.7  5.5   \n",
      "24  1972   6.5   6.4   6.1  5.5  5.1  6.2  5.9  5.5  5.4  5.1  4.9  4.8   \n",
      "25  1973   5.5   5.6   5.2  4.8  4.4  5.4  5.0  4.7  4.7  4.2  4.6  4.6   \n",
      "26  1974   5.7   5.8   5.3  4.8  4.6  5.8  5.7  5.3  5.7  5.5  6.2  6.7   \n",
      "27  1975   9.0   9.1   9.1  8.6  8.3  9.1  8.7  8.2  8.1  7.8  7.8  7.8   \n",
      "28  1976   8.8   8.7   8.1  7.4  6.8  8.0  7.8  7.6  7.4  7.2  7.4  7.4   \n",
      "29  1977   8.3   8.5   7.9  6.9  6.4  7.5  7.0  6.8  6.6  6.4  6.5  6.0   \n",
      "..   ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "36  1984   8.8   8.4   8.1  7.6  7.2  7.4  7.5  7.3  7.1  7.0  6.9  7.0   \n",
      "37  1985   8.0   7.8   7.5  7.1  7.0  7.5  7.4  6.9  6.9  6.8  6.7  6.7   \n",
      "38  1986   7.3   7.8   7.5  7.0  7.0  7.3  7.0  6.7  6.8  6.6  6.6  6.3   \n",
      "39  1987   7.3   7.2   6.9  6.2  6.1  6.3  6.1  5.8  5.7  5.7  5.6  5.4   \n",
      "40  1988   6.3   6.2   5.9  5.3  5.4  5.5  5.5  5.4  5.2  5.0  5.2  5.0   \n",
      "41  1989   6.0   5.6   5.2  5.1  5.0  5.5  5.3  5.1  5.1  5.0  5.2  5.1   \n",
      "42  1990   6.0   5.9   5.5  5.3  5.2  5.4  5.6  5.5  5.6  5.5  5.9  6.0   \n",
      "43  1991   7.1   7.3   7.2  6.5  6.7  7.0  6.8  6.6  6.5  6.5  6.7  6.9   \n",
      "44  1992   8.1   8.2   7.8  7.2  7.3  8.0  7.7  7.4  7.3  6.9  7.1  7.1   \n",
      "45  1993   8.0   7.8   7.4  6.9  6.8  7.2  7.0  6.6  6.4  6.4  6.2  6.1   \n",
      "46  1994   7.3   7.1   6.8  6.2  5.9  6.2  6.2  5.9  5.6  5.4  5.3  5.1   \n",
      "47  1995   6.2   5.9   5.7  5.6  5.5  5.8  5.9  5.6  5.4  5.2  5.3  5.2   \n",
      "48  1996   6.3   6.0   5.8  5.4  5.4  5.5  5.6  5.1  5.0  4.9  5.0  5.0   \n",
      "49  1997   5.9   5.7   5.5  4.8  4.7  5.2  5.0  4.8  4.7  4.4  4.3  4.4   \n",
      "50  1998   5.2   5.0   5.0  4.1  4.2  4.7  4.7  4.5  4.4  4.2  4.1  4.0   \n",
      "51  1999   4.8   4.7   4.4  4.1  4.0  4.5  4.5  4.2  4.1  3.8  3.8  3.7   \n",
      "52  2000   4.5   4.4   4.3  3.7  3.8  4.1  4.2  4.1  3.8  3.6  3.7  3.7   \n",
      "53  2001   4.7   4.6   4.5  4.2  4.1  4.7  4.7  4.9  4.7  5.0  5.3  5.4   \n",
      "54  2002   6.3   6.1   6.1  5.7  5.5  6.0  5.9  5.7  5.4  5.3  5.6  5.7   \n",
      "55  2003   6.5   6.4   6.2  5.8  5.8  6.5  6.3  6.0  5.8  5.6  5.6  5.4   \n",
      "56  2004   6.3   6.0   6.0  5.4  5.3  5.8  5.7  5.4  5.1  5.1  5.2  5.1   \n",
      "57  2005   5.7   5.8   5.4  4.9  4.9  5.2  5.2  4.9  4.8  4.6  4.8  4.6   \n",
      "58  2006   5.1   5.1   4.8  4.5  4.4  4.8  5.0  4.6  4.4  4.1  4.3  4.3   \n",
      "59  2007   5.0   4.9   4.5  4.3  4.3  4.7  4.9  4.6  4.5  4.4  4.5  4.8   \n",
      "60  2008   5.4   5.2   5.2  4.8  5.2  5.7  6.0  6.1  6.0  6.1  6.5  7.1   \n",
      "61  2009   8.5   8.9   9.0  8.6  9.1  9.7  9.7  9.6  9.5  9.5  9.4  9.7   \n",
      "62  2010  10.6  10.4  10.2  9.5  9.3  9.6  9.7  9.5  9.2  9.0  9.3  9.1   \n",
      "63  2011   9.8   9.5   9.2  8.7  8.7  9.3  9.3  9.1  8.8  8.5  8.2  8.3   \n",
      "64  2012   8.8   8.7   8.4  7.7  7.9  8.4  8.6  8.2  7.6  7.5  7.4  7.6   \n",
      "65  2013   8.5   8.1   7.6  7.1  7.3  7.8  7.7  7.3  7.0  7.0  6.6  6.5   \n",
      "\n",
      "    Annual  \n",
      "0      3.8  \n",
      "1      5.9  \n",
      "2      5.3  \n",
      "3      3.3  \n",
      "4      3.0  \n",
      "5      2.9  \n",
      "6      5.5  \n",
      "7      4.4  \n",
      "8      4.1  \n",
      "9      4.3  \n",
      "10     6.8  \n",
      "11     5.5  \n",
      "12     5.5  \n",
      "13     6.7  \n",
      "14     5.5  \n",
      "15     5.7  \n",
      "16     5.2  \n",
      "17     4.5  \n",
      "18     3.8  \n",
      "19     3.8  \n",
      "20     3.6  \n",
      "21     3.5  \n",
      "22     4.9  \n",
      "23     5.9  \n",
      "24     5.6  \n",
      "25     4.9  \n",
      "26     5.6  \n",
      "27     8.5  \n",
      "28     7.7  \n",
      "29     7.1  \n",
      "..     ...  \n",
      "36     7.5  \n",
      "37     7.2  \n",
      "38     7.0  \n",
      "39     6.2  \n",
      "40     5.5  \n",
      "41     5.3  \n",
      "42     5.6  \n",
      "43     6.8  \n",
      "44     7.5  \n",
      "45     6.9  \n",
      "46     6.1  \n",
      "47     5.6  \n",
      "48     5.4  \n",
      "49     4.9  \n",
      "50     4.5  \n",
      "51     4.2  \n",
      "52     4.0  \n",
      "53     4.7  \n",
      "54     5.8  \n",
      "55     6.0  \n",
      "56     5.5  \n",
      "57     5.1  \n",
      "58     4.6  \n",
      "59     4.6  \n",
      "60     5.8  \n",
      "61     9.3  \n",
      "62     9.6  \n",
      "63     8.9  \n",
      "64     8.1  \n",
      "65     7.4  \n",
      "\n",
      "[66 rows x 14 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"plotdiv\" id=\"67808442-f45d-4f38-be85-69f9c520d287\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"67808442-f45d-4f38-be85-69f9c520d287\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid '67808442-f45d-4f38-be85-69f9c520d287' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        Bokeh.$(function() {\n",
       "            var docs_json = {\"ae99b0e0-1222-4ca1-b0b5-1f9f319f8e43\":{\"roots\":{\"references\":[{\"attributes\":{\"dilate\":true,\"fill_color\":{\"value\":\"#08519c\"},\"height\":{\"units\":\"data\",\"value\":0.95},\"line_color\":{\"value\":\"#08519c\"},\"width\":{\"units\":\"data\",\"value\":0.95},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"cdff79bc-06dd-4d5e-bfe9-927280ba9931\",\"type\":\"Rect\"},{\"attributes\":{\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"d69ea053-3a45-42e4-a026-d1d356b1501b\",\"type\":\"PreviewSaveTool\"},{\"attributes\":{},\"id\":\"c329f8ed-f6b1-41d6-a11b-bc5113419ec5\",\"type\":\"ToolEvents\"},{\"attributes\":{\"axis_label\":\"1948\",\"formatter\":{\"id\":\"b45bc6ac-5aac-4850-a6e5-0734b32384b4\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"8e00cea3-31e7-478c-982d-1a3ca5139c44\",\"type\":\"BasicTicker\"}},\"id\":\"0341d190-8f16-425d-90a4-2268b4c38a74\",\"type\":\"LinearAxis\"},{\"attributes\":{\"below\":[{\"id\":\"0341d190-8f16-425d-90a4-2268b4c38a74\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"b5b840a7-75b6-4e68-956e-4df0ed8bd049\",\"type\":\"LinearAxis\"}],\"legend\":null,\"plot_width\":800,\"renderers\":[{\"id\":\"37e4407a-6911-4e2b-9d66-3afca326375d\",\"type\":\"BoxAnnotation\"},{\"id\":\"9ffe58c7-4255-465e-90db-2b24f9ca9c31\",\"type\":\"GlyphRenderer\"},{\"id\":\"0341d190-8f16-425d-90a4-2268b4c38a74\",\"type\":\"LinearAxis\"},{\"id\":\"b5b840a7-75b6-4e68-956e-4df0ed8bd049\",\"type\":\"LinearAxis\"}],\"title\":\"categorical heatmap\",\"title_text_font_size\":{\"value\":\"14pt\"},\"tool_events\":{\"id\":\"c329f8ed-f6b1-41d6-a11b-bc5113419ec5\",\"type\":\"ToolEvents\"},\"tools\":[{\"id\":\"9ebfb01e-8118-44c1-bc6f-680fb073b86b\",\"type\":\"PanTool\"},{\"id\":\"d40629e9-10de-40cb-ab27-ee2b3eb1baac\",\"type\":\"WheelZoomTool\"},{\"id\":\"33ede7bc-4452-4d6d-adf3-d4b3e0189efc\",\"type\":\"BoxZoomTool\"},{\"id\":\"d69ea053-3a45-42e4-a026-d1d356b1501b\",\"type\":\"PreviewSaveTool\"},{\"id\":\"86a23a16-b3f2-4f88-be07-4f904556e7b0\",\"type\":\"ResizeTool\"},{\"id\":\"71768278-1ac3-40a2-be40-77aa7053bc54\",\"type\":\"ResetTool\"},{\"id\":\"09d343a2-9bda-48dd-b2fe-f7eb0906282f\",\"type\":\"HelpTool\"}],\"x_mapper_type\":\"auto\",\"x_range\":{\"id\":\"27635836-4ab9-4049-9582-88fdc2742a48\",\"type\":\"Range1d\"},\"xgrid\":false,\"xscale\":\"auto\",\"y_mapper_type\":\"auto\",\"y_range\":{\"id\":\"36706df1-1dd6-4624-8b11-0a000d8d0094\",\"type\":\"Range1d\"},\"ygrid\":false,\"yscale\":\"auto\"},\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"37e4407a-6911-4e2b-9d66-3afca326375d\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"callback\":null,\"end\":5.45,\"start\":2.15},\"id\":\"27635836-4ab9-4049-9582-88fdc2742a48\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"f49dccbc-d912-427b-8699-7bb320697c37\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"end\":7.77,\"start\":4.23},\"id\":\"36706df1-1dd6-4624-8b11-0a000d8d0094\",\"type\":\"Range1d\"},{\"attributes\":{\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"09d343a2-9bda-48dd-b2fe-f7eb0906282f\",\"type\":\"HelpTool\"},{\"attributes\":{\"overlay\":{\"id\":\"37e4407a-6911-4e2b-9d66-3afca326375d\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"33ede7bc-4452-4d6d-adf3-d4b3e0189efc\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"86a23a16-b3f2-4f88-be07-4f904556e7b0\",\"type\":\"ResizeTool\"},{\"attributes\":{\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"71768278-1ac3-40a2-be40-77aa7053bc54\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"8e00cea3-31e7-478c-982d-1a3ca5139c44\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"823db36f-25ac-4fbb-adf0-80eeb623f931\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"cdff79bc-06dd-4d5e-bfe9-927280ba9931\",\"type\":\"Rect\"},\"hover_glyph\":null,\"nonselection_glyph\":null,\"selection_glyph\":null},\"id\":\"9ffe58c7-4255-465e-90db-2b24f9ca9c31\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"0d4e45fa-42bb-4840-9618-053134c869fe\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"9ebfb01e-8118-44c1-bc6f-680fb073b86b\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"b45bc6ac-5aac-4850-a6e5-0734b32384b4\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"d40629e9-10de-40cb-ab27-ee2b3eb1baac\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\",\"values\"],\"data\":{\"1950_Count\":[\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\",\"(0.9997, 1]\"],\"chart_index\":[{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"},{\"1950_Count\":\"(0.9997, 1]\"}],\"values\":[1,1,1,1,1,1,1,1,1,1,1],\"x\":[4.0,4.7,4.5,4.0,3.4,3.9,3.9,3.6,3.4,2.9,3.3],\"y\":[5.0,5.8,5.6,5.4,5.7,6.4,7.0,6.3,5.9,6.1,5.7]}},\"id\":\"823db36f-25ac-4fbb-adf0-80eeb623f931\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis_label\":\"1949\",\"formatter\":{\"id\":\"0d4e45fa-42bb-4840-9618-053134c869fe\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"f49dccbc-d912-427b-8699-7bb320697c37\",\"type\":\"BasicTicker\"}},\"id\":\"b5b840a7-75b6-4e68-956e-4df0ed8bd049\",\"type\":\"LinearAxis\"}],\"root_ids\":[\"7045e5fc-aed2-4668-a481-b87daaf1db54\"]},\"title\":\"Bokeh Application\",\"version\":\"0.11.1\"}};\n",
       "            var render_items = [{\"docid\":\"ae99b0e0-1222-4ca1-b0b5-1f9f319f8e43\",\"elementid\":\"67808442-f45d-4f38-be85-69f9c520d287\",\"modelid\":\"7045e5fc-aed2-4668-a481-b87daaf1db54\",\"notebook_comms_target\":\"6be4a573-5e54-41ac-8871-dad01d57adb4\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        });\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }\n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><code>&lt;Bokeh Notebook handle for <strong>In[1]</strong>&gt;</code></p>"
      ],
      "text/plain": [
       "<bokeh.io._CommsHandle at 0x116045ed0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_data(\"train.csv\")\n",
    "\n",
    "correlations = pd.DataFrame(train_df.corr())\n",
    "print(correlations)\n",
    "target_correl = correlations['target']\n",
    "#print(target_correl.order())\n",
    "\n",
    "from bokeh.charts import HeatMap, output_file, show\n",
    "from bokeh.sampledata.unemployment1948 import data\n",
    "\n",
    "# pandas magic\n",
    "print(data)\n",
    "\n",
    "df = data[data.columns[:-2]]\n",
    "df2 = df.set_index(df[df.columns[0]].astype(str))\n",
    "df2.drop(df.columns[0], axis=1, inplace=True)\n",
    "df3 = df2.transpose()\n",
    "\n",
    "output_file(\"cat_heatmap.html\")\n",
    "\n",
    "hm = HeatMap(df3, title=\"categorical heatmap\", width=800)\n",
    "\n",
    "show(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
